{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T01:55:49.664531Z",
     "iopub.status.busy": "2026-01-28T01:55:49.664183Z",
     "iopub.status.idle": "2026-01-28T01:57:40.648500Z",
     "shell.execute_reply": "2026-01-28T01:57:40.647113Z",
     "shell.execute_reply.started": "2026-01-28T01:55:49.664429Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "\n",
    "import warnings, logging, os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "for logger_name in ['nemo_logger', 'nemo', 'pytorch_lightning', 'pyannote', 'numba']:\n",
    "    logging.getLogger(logger_name).setLevel(logging.ERROR)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "\n",
    "# === 1. Install Dependencies ===\n",
    "# (Using -q for quiet installation to keep the cell clean)\n",
    "!pip install wget text-unidecode -q\n",
    "!apt-get install -y sox libsndfile1 ffmpeg -q\n",
    "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@r1.23.0#egg=nemo_toolkit[asr] -q\n",
    "!pip install torchaudio -f https://download.pytorch.org/whl/torch_stable.html -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T01:57:40.651144Z",
     "iopub.status.busy": "2026-01-28T01:57:40.650857Z",
     "iopub.status.idle": "2026-01-28T01:57:59.962638Z",
     "shell.execute_reply": "2026-01-28T01:57:59.961864Z",
     "shell.execute_reply.started": "2026-01-28T01:57:40.651119Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === 2. Imports ===\n",
    "import glob, json, wget, librosa, pprint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from omegaconf import OmegaConf\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "import nemo\n",
    "import nemo.collections.asr as nemo_asr\n",
    "from nemo.collections.asr.parts.utils.decoder_timestamps_utils import ASRDecoderTimeStamps\n",
    "from nemo.collections.asr.parts.utils.diarization_utils import OfflineDiarWithASR\n",
    "from nemo.collections.asr.parts.utils.speaker_utils import rttm_to_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T01:57:59.964215Z",
     "iopub.status.busy": "2026-01-28T01:57:59.963710Z",
     "iopub.status.idle": "2026-01-28T01:57:59.977130Z",
     "shell.execute_reply": "2026-01-28T01:57:59.976354Z",
     "shell.execute_reply.started": "2026-01-28T01:57:59.964192Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === 3. Configuration & Setup ===\n",
    "# Paths provided by the user\n",
    "AUDIO_FILENAME = '/kaggle/input/voxconverse-dataset/voxconverse_dev_wav/audio/afjiv.wav'\n",
    "RTTM_FILEPATH = '/kaggle/input/voxconverse-dataset/labels/dev/afjiv.rttm'\n",
    "\n",
    "# Setup local workspace\n",
    "ROOT = os.getcwd()\n",
    "data_dir = os.path.join(ROOT, 'data')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Safety check for audio file\n",
    "if not os.path.exists(AUDIO_FILENAME):\n",
    "    raise FileNotFoundError(f\"CRITICAL: Audio file not found at {AUDIO_FILENAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T01:57:59.978769Z",
     "iopub.status.busy": "2026-01-28T01:57:59.978493Z",
     "iopub.status.idle": "2026-01-28T01:58:07.022564Z",
     "shell.execute_reply": "2026-01-28T01:58:07.021768Z",
     "shell.execute_reply.started": "2026-01-28T01:57:59.978749Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: afjiv.wav\n"
     ]
    }
   ],
   "source": [
    "# === 5. Load Audio ===\n",
    "print(f\"Loading: {os.path.basename(AUDIO_FILENAME)}\")\n",
    "signal, sample_rate = librosa.load(AUDIO_FILENAME, sr=None)\n",
    "\n",
    "# === 6. Configure Diarization System ===\n",
    "# Download the standard meeting inference config\n",
    "CONFIG_URL = \"https://raw.githubusercontent.com/NVIDIA/NeMo/main/examples/speaker_tasks/diarization/conf/inference/diar_infer_meeting.yaml\"\n",
    "CONFIG_PATH = os.path.join(data_dir, \"diar_config.yaml\")\n",
    "\n",
    "if not os.path.exists(CONFIG_PATH):\n",
    "    CONFIG_PATH = wget.download(CONFIG_URL, data_dir)\n",
    "\n",
    "cfg = OmegaConf.load(CONFIG_PATH)\n",
    "\n",
    "# Prepare Manifest with Ground Truth RTTM\n",
    "meta = {\n",
    "    'audio_filepath': AUDIO_FILENAME,\n",
    "    'offset': 0,\n",
    "    'duration': None,\n",
    "    'label': 'infer',\n",
    "    'text': '-',\n",
    "    'num_speakers': None,\n",
    "    'rttm_filepath': RTTM_FILEPATH if os.path.exists(RTTM_FILEPATH) else None,\n",
    "    'uem_filepath': None\n",
    "}\n",
    "\n",
    "manifest_path = os.path.join(data_dir, 'input_manifest.json')\n",
    "with open(manifest_path, 'w') as f:\n",
    "    json.dump(meta, f)\n",
    "    f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T01:58:07.024825Z",
     "iopub.status.busy": "2026-01-28T01:58:07.024336Z",
     "iopub.status.idle": "2026-01-28T01:58:07.030375Z",
     "shell.execute_reply": "2026-01-28T01:58:07.029504Z",
     "shell.execute_reply.started": "2026-01-28T01:58:07.024803Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# High-Accuracy Model Settings\n",
    "cfg.diarizer.manifest_filepath = manifest_path\n",
    "cfg.diarizer.out_dir = data_dir\n",
    "cfg.diarizer.speaker_embeddings.model_path = 'titanet_large'\n",
    "cfg.diarizer.vad.model_path = 'vad_multilingual_marblenet'\n",
    "\n",
    "# IMPROVEMENT: Upgrading ASR model for better accuracy\n",
    "cfg.diarizer.asr.model_path = 'stt_en_conformer_ctc_large' \n",
    "\n",
    "cfg.diarizer.oracle_vad = False\n",
    "cfg.diarizer.asr.parameters.asr_based_vad = False\n",
    "cfg.diarizer.clustering.parameters.oracle_num_speakers = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T01:58:07.031691Z",
     "iopub.status.busy": "2026-01-28T01:58:07.031467Z",
     "iopub.status.idle": "2026-01-28T01:58:30.861788Z",
     "shell.execute_reply": "2026-01-28T01:58:30.860768Z",
     "shell.execute_reply.started": "2026-01-28T01:58:07.031673Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2026-01-28 01:58:07 speaker_utils:93] Number of files to diarize: 1\n",
      "[NeMo I 2026-01-28 01:58:07 cloud:68] Downloading from: https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_en_conformer_ctc_large/versions/1.10.0/files/stt_en_conformer_ctc_large.nemo to /root/.cache/torch/NeMo/NeMo_1.23.0/stt_en_conformer_ctc_large/afb212c5bcf904e326b5e5751e7c7465/stt_en_conformer_ctc_large.nemo\n",
      "[NeMo I 2026-01-28 01:58:09 common:924] Instantiating model from pre-trained checkpoint\n",
      "[NeMo I 2026-01-28 01:58:09 mixins:172] Tokenizer SentencePieceTokenizer initialized with 128 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2026-01-28 01:58:09 modelPT:165] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath:\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket1/tarred_audio_manifest.json\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket2/tarred_audio_manifest.json\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket3/tarred_audio_manifest.json\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket4/tarred_audio_manifest.json\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket5/tarred_audio_manifest.json\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket6/tarred_audio_manifest.json\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket7/tarred_audio_manifest.json\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket8/tarred_audio_manifest.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 1\n",
      "    shuffle: true\n",
      "    num_workers: 4\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    trim_silence: false\n",
      "    max_duration: 20.0\n",
      "    min_duration: 0.1\n",
      "    is_tarred: true\n",
      "    tarred_audio_filepaths:\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket1/audio__OP_0..8191_CL_.tar\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket2/audio__OP_0..8191_CL_.tar\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket3/audio__OP_0..8191_CL_.tar\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket4/audio__OP_0..8191_CL_.tar\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket5/audio__OP_0..8191_CL_.tar\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket6/audio__OP_0..8191_CL_.tar\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket7/audio__OP_0..8191_CL_.tar\n",
      "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket8/audio__OP_0..8191_CL_.tar\n",
      "    shuffle_n: 2048\n",
      "    bucketing_strategy: synced_randomized\n",
      "    bucketing_batch_size:\n",
      "    - 34\n",
      "    - 30\n",
      "    - 26\n",
      "    - 22\n",
      "    - 18\n",
      "    - 16\n",
      "    - 12\n",
      "    - 8\n",
      "    \n",
      "[NeMo W 2026-01-28 01:58:09 modelPT:172] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath:\n",
      "    - /manifests/librispeech/librivox-dev-other.json\n",
      "    - /manifests/librispeech/librivox-dev-clean.json\n",
      "    - /manifests/librispeech/librivox-test-other.json\n",
      "    - /manifests/librispeech/librivox-test-clean.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    \n",
      "[NeMo W 2026-01-28 01:58:09 modelPT:178] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath:\n",
      "    - /manifests/librispeech/librivox-dev-other.json\n",
      "    - /manifests/librispeech/librivox-dev-clean.json\n",
      "    - /manifests/librispeech/librivox-test-other.json\n",
      "    - /manifests/librispeech/librivox-test-clean.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2026-01-28 01:58:09 features:289] PADDING: 0\n",
      "[NeMo I 2026-01-28 01:58:12 save_restore_connector:249] Model EncDecCTCModelBPE was successfully restored from /root/.cache/torch/NeMo/NeMo_1.23.0/stt_en_conformer_ctc_large/afb212c5bcf904e326b5e5751e7c7465/stt_en_conformer_ctc_large.nemo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2026-01-28 01:58:12 decoder_timestamps_utils:71] `ctc_decode` was set to True. Note that this is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2026-01-28 01:58:12 features:289] PADDING: 0\n",
      "[NeMo I 2026-01-28 01:58:12 features:289] PADDING: 0\n",
      "[NeMo I 2026-01-28 01:58:12 decoder_timestamps_utils:656] Running ASR model stt_en_conformer_ctc_large\n",
      "[NeMo I 2026-01-28 01:58:12 decoder_timestamps_utils:660] [1/1] FrameBatchASR: /kaggle/input/voxconverse-dataset/voxconverse_dev_wav/audio/afjiv.wav\n",
      "[NeMo I 2026-01-28 01:58:15 speaker_utils:93] Number of files to diarize: 1\n",
      "[NeMo I 2026-01-28 01:58:15 clustering_diarizer:127] Loading pretrained vad_multilingual_marblenet model from NGC\n",
      "[NeMo I 2026-01-28 01:58:15 cloud:68] Downloading from: https://api.ngc.nvidia.com/v2/models/nvidia/nemo/vad_multilingual_marblenet/versions/1.10.0/files/vad_multilingual_marblenet.nemo to /root/.cache/torch/NeMo/NeMo_1.23.0/vad_multilingual_marblenet/670f425c7f186060b7a7268ba6dfacb2/vad_multilingual_marblenet.nemo\n",
      "[NeMo I 2026-01-28 01:58:22 common:924] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2026-01-28 01:58:22 modelPT:165] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /manifests/ami_train_0.63.json,/manifests/freesound_background_train.json,/manifests/freesound_laughter_train.json,/manifests/fisher_2004_background.json,/manifests/fisher_2004_speech_sampled.json,/manifests/google_train_manifest.json,/manifests/icsi_all_0.63.json,/manifests/musan_freesound_train.json,/manifests/musan_music_train.json,/manifests/musan_soundbible_train.json,/manifests/mandarin_train_sample.json,/manifests/german_train_sample.json,/manifests/spanish_train_sample.json,/manifests/french_train_sample.json,/manifests/russian_train_sample.json\n",
      "    sample_rate: 16000\n",
      "    labels:\n",
      "    - background\n",
      "    - speech\n",
      "    batch_size: 256\n",
      "    shuffle: true\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: null\n",
      "    tarred_shard_strategy: scatter\n",
      "    augmentor:\n",
      "      shift:\n",
      "        prob: 0.5\n",
      "        min_shift_ms: -10.0\n",
      "        max_shift_ms: 10.0\n",
      "      white_noise:\n",
      "        prob: 0.5\n",
      "        min_level: -90\n",
      "        max_level: -46\n",
      "        norm: true\n",
      "      noise:\n",
      "        prob: 0.5\n",
      "        manifest_path: /manifests/noise_0_1_musan_fs.json\n",
      "        min_snr_db: 0\n",
      "        max_snr_db: 30\n",
      "        max_gain_db: 300.0\n",
      "        norm: true\n",
      "      gain:\n",
      "        prob: 0.5\n",
      "        min_gain_dbfs: -10.0\n",
      "        max_gain_dbfs: 10.0\n",
      "        norm: true\n",
      "    num_workers: 16\n",
      "    pin_memory: true\n",
      "    \n",
      "[NeMo W 2026-01-28 01:58:22 modelPT:172] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: /manifests/ami_dev_0.63.json,/manifests/freesound_background_dev.json,/manifests/freesound_laughter_dev.json,/manifests/ch120_moved_0.63.json,/manifests/fisher_2005_500_speech_sampled.json,/manifests/google_dev_manifest.json,/manifests/musan_music_dev.json,/manifests/mandarin_dev.json,/manifests/german_dev.json,/manifests/spanish_dev.json,/manifests/french_dev.json,/manifests/russian_dev.json\n",
      "    sample_rate: 16000\n",
      "    labels:\n",
      "    - background\n",
      "    - speech\n",
      "    batch_size: 256\n",
      "    shuffle: false\n",
      "    val_loss_idx: 0\n",
      "    num_workers: 16\n",
      "    pin_memory: true\n",
      "    \n",
      "[NeMo W 2026-01-28 01:58:22 modelPT:178] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    labels:\n",
      "    - background\n",
      "    - speech\n",
      "    batch_size: 128\n",
      "    shuffle: false\n",
      "    test_loss_idx: 0\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2026-01-28 01:58:22 features:289] PADDING: 16\n",
      "[NeMo I 2026-01-28 01:58:22 save_restore_connector:249] Model EncDecClassificationModel was successfully restored from /root/.cache/torch/NeMo/NeMo_1.23.0/vad_multilingual_marblenet/670f425c7f186060b7a7268ba6dfacb2/vad_multilingual_marblenet.nemo.\n",
      "[NeMo I 2026-01-28 01:58:22 clustering_diarizer:157] Loading pretrained titanet_large model from NGC\n",
      "[NeMo I 2026-01-28 01:58:22 cloud:68] Downloading from: https://api.ngc.nvidia.com/v2/models/nvidia/nemo/titanet_large/versions/v1/files/titanet-l.nemo to /root/.cache/torch/NeMo/NeMo_1.23.0/titanet-l/11ba0924fdf87c049e339adbf6899d48/titanet-l.nemo\n",
      "[NeMo I 2026-01-28 01:58:23 common:924] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2026-01-28 01:58:23 modelPT:165] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /manifests/combined_fisher_swbd_voxceleb12_librispeech/train.json\n",
      "    sample_rate: 16000\n",
      "    labels: null\n",
      "    batch_size: 64\n",
      "    shuffle: true\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: null\n",
      "    tarred_shard_strategy: scatter\n",
      "    augmentor:\n",
      "      noise:\n",
      "        manifest_path: /manifests/noise/rir_noise_manifest.json\n",
      "        prob: 0.5\n",
      "        min_snr_db: 0\n",
      "        max_snr_db: 15\n",
      "      speed:\n",
      "        prob: 0.5\n",
      "        sr: 16000\n",
      "        resample_type: kaiser_fast\n",
      "        min_speed_rate: 0.95\n",
      "        max_speed_rate: 1.05\n",
      "    num_workers: 15\n",
      "    pin_memory: true\n",
      "    \n",
      "[NeMo W 2026-01-28 01:58:23 modelPT:172] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: /manifests/combined_fisher_swbd_voxceleb12_librispeech/dev.json\n",
      "    sample_rate: 16000\n",
      "    labels: null\n",
      "    batch_size: 128\n",
      "    shuffle: false\n",
      "    num_workers: 15\n",
      "    pin_memory: true\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2026-01-28 01:58:24 features:289] PADDING: 16\n",
      "[NeMo I 2026-01-28 01:58:24 save_restore_connector:249] Model EncDecSpeakerLabelModel was successfully restored from /root/.cache/torch/NeMo/NeMo_1.23.0/titanet-l/11ba0924fdf87c049e339adbf6899d48/titanet-l.nemo.\n",
      "[NeMo I 2026-01-28 01:58:24 speaker_utils:93] Number of files to diarize: 1\n",
      "[NeMo I 2026-01-28 01:58:24 clustering_diarizer:309] Split long audio file to avoid CUDA memory issue\n",
      "[NeMo I 2026-01-28 01:58:24 classification_models:273] Perform streaming frame-level VAD\n",
      "[NeMo I 2026-01-28 01:58:24 collections:445] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2026-01-28 01:58:24 collections:446] Dataset loaded with 4 items, total duration of  0.04 hours.\n",
      "[NeMo I 2026-01-28 01:58:24 collections:448] # 4 files loaded accounting to # 1 labels\n",
      "[NeMo I 2026-01-28 01:58:25 clustering_diarizer:262] Converting frame level prediction to speech/no-speech segment in start and end times format.\n",
      "[NeMo I 2026-01-28 01:58:26 clustering_diarizer:287] Subsegmentation for embedding extraction: scale0, /kaggle/working/data/speaker_outputs/subsegments_scale0.json\n",
      "[NeMo I 2026-01-28 01:58:26 clustering_diarizer:343] Extracting embeddings for Diarization\n",
      "[NeMo I 2026-01-28 01:58:26 collections:445] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2026-01-28 01:58:26 collections:446] Dataset loaded with 89 items, total duration of  0.07 hours.\n",
      "[NeMo I 2026-01-28 01:58:26 collections:448] # 89 files loaded accounting to # 1 labels\n",
      "[NeMo I 2026-01-28 01:58:26 clustering_diarizer:389] Saved embedding files to /kaggle/working/data/speaker_outputs/embeddings\n",
      "[NeMo I 2026-01-28 01:58:26 clustering_diarizer:287] Subsegmentation for embedding extraction: scale1, /kaggle/working/data/speaker_outputs/subsegments_scale1.json\n",
      "[NeMo I 2026-01-28 01:58:26 clustering_diarizer:343] Extracting embeddings for Diarization\n",
      "[NeMo I 2026-01-28 01:58:26 collections:445] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2026-01-28 01:58:26 collections:446] Dataset loaded with 108 items, total duration of  0.07 hours.\n",
      "[NeMo I 2026-01-28 01:58:26 collections:448] # 108 files loaded accounting to # 1 labels\n",
      "[NeMo I 2026-01-28 01:58:27 clustering_diarizer:389] Saved embedding files to /kaggle/working/data/speaker_outputs/embeddings\n",
      "[NeMo I 2026-01-28 01:58:27 clustering_diarizer:287] Subsegmentation for embedding extraction: scale2, /kaggle/working/data/speaker_outputs/subsegments_scale2.json\n",
      "[NeMo I 2026-01-28 01:58:27 clustering_diarizer:343] Extracting embeddings for Diarization\n",
      "[NeMo I 2026-01-28 01:58:27 collections:445] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2026-01-28 01:58:27 collections:446] Dataset loaded with 134 items, total duration of  0.07 hours.\n",
      "[NeMo I 2026-01-28 01:58:27 collections:448] # 134 files loaded accounting to # 1 labels\n",
      "[NeMo I 2026-01-28 01:58:27 clustering_diarizer:389] Saved embedding files to /kaggle/working/data/speaker_outputs/embeddings\n",
      "[NeMo I 2026-01-28 01:58:27 clustering_diarizer:287] Subsegmentation for embedding extraction: scale3, /kaggle/working/data/speaker_outputs/subsegments_scale3.json\n",
      "[NeMo I 2026-01-28 01:58:27 clustering_diarizer:343] Extracting embeddings for Diarization\n",
      "[NeMo I 2026-01-28 01:58:27 collections:445] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2026-01-28 01:58:27 collections:446] Dataset loaded with 181 items, total duration of  0.07 hours.\n",
      "[NeMo I 2026-01-28 01:58:27 collections:448] # 181 files loaded accounting to # 1 labels\n",
      "[NeMo I 2026-01-28 01:58:28 clustering_diarizer:389] Saved embedding files to /kaggle/working/data/speaker_outputs/embeddings\n",
      "[NeMo I 2026-01-28 01:58:28 clustering_diarizer:287] Subsegmentation for embedding extraction: scale4, /kaggle/working/data/speaker_outputs/subsegments_scale4.json\n",
      "[NeMo I 2026-01-28 01:58:28 clustering_diarizer:343] Extracting embeddings for Diarization\n",
      "[NeMo I 2026-01-28 01:58:28 collections:445] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2026-01-28 01:58:28 collections:446] Dataset loaded with 272 items, total duration of  0.08 hours.\n",
      "[NeMo I 2026-01-28 01:58:28 collections:448] # 272 files loaded accounting to # 1 labels\n",
      "[NeMo I 2026-01-28 01:58:28 clustering_diarizer:389] Saved embedding files to /kaggle/working/data/speaker_outputs/embeddings\n",
      "[NeMo I 2026-01-28 01:58:28 clustering_diarizer:287] Subsegmentation for embedding extraction: scale5, /kaggle/working/data/speaker_outputs/subsegments_scale5.json\n",
      "[NeMo I 2026-01-28 01:58:28 clustering_diarizer:343] Extracting embeddings for Diarization\n",
      "[NeMo I 2026-01-28 01:58:28 collections:445] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2026-01-28 01:58:28 collections:446] Dataset loaded with 546 items, total duration of  0.08 hours.\n",
      "[NeMo I 2026-01-28 01:58:28 collections:448] # 546 files loaded accounting to # 1 labels\n",
      "[NeMo I 2026-01-28 01:58:29 clustering_diarizer:389] Saved embedding files to /kaggle/working/data/speaker_outputs/embeddings\n",
      "[NeMo I 2026-01-28 01:58:30 clustering_diarizer:464] Outputs are saved in /kaggle/working/data directory\n",
      "[NeMo I 2026-01-28 01:58:30 der:176] Cumulative Results for collar 0.25 sec and ignore_overlap True: \n",
      "     FA: 0.0315\t MISS 0.0000\t                 Diarization ER: 0.0318\t, Confusion ER:0.0002\n",
      "[NeMo I 2026-01-28 01:58:30 diarization_utils:876] Creating results for Session: afjiv n_spk: 5 \n",
      "[NeMo I 2026-01-28 01:58:30 diarization_utils:749] Diarization with ASR output files are saved in: /kaggle/working/data/pred_rttms\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "# === Run Inference ===\n",
    "AUDIO_FILE = '/kaggle/input/voxconverse-dataset/voxconverse_dev_wav/audio/afjiv.wav'\n",
    "RTTM_FILE = '/kaggle/input/voxconverse-dataset/labels/dev/afjiv.rttm'\n",
    "asr_decoder_ts = ASRDecoderTimeStamps(cfg.diarizer)\n",
    "asr_model = asr_decoder_ts.set_asr_model()\n",
    "word_hyp, word_ts_hyp = asr_decoder_ts.run_ASR(asr_model)\n",
    "\n",
    "asr_diar_offline = OfflineDiarWithASR(cfg.diarizer)\n",
    "asr_diar_offline.word_ts_anchor_offset = asr_decoder_ts.word_ts_anchor_offset\n",
    "diar_hyp, diar_score = asr_diar_offline.run_diarization(cfg, word_ts_hyp)\n",
    "\n",
    "# === Results ===\n",
    "trans_info_dict = asr_diar_offline.get_transcript_with_speaker_labels(diar_hyp, word_hyp, word_ts_hyp)\n",
    "file_key = os.path.splitext(os.path.basename(AUDIO_FILE))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T01:58:30.863589Z",
     "iopub.status.busy": "2026-01-28T01:58:30.863319Z",
     "iopub.status.idle": "2026-01-28T01:58:30.871939Z",
     "shell.execute_reply": "2026-01-28T01:58:30.871141Z",
     "shell.execute_reply.started": "2026-01-28T01:58:30.863565Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index     Start      End Speaker      Word\n",
      "------------------------------------------------------------\n",
      "0          5.20     5.24 speaker_2    i\n",
      "1          5.32     5.48 speaker_2    think\n",
      "2          5.72     5.84 speaker_2    if\n",
      "3          5.96     6.16 speaker_2    you're\n",
      "4          6.24     6.28 speaker_2    a\n",
      "5          6.40     6.64 speaker_2    leader\n",
      "6          6.68     6.72 speaker_2    and\n",
      "7          6.80     6.84 speaker_2    you\n",
      "8          6.88     7.08 speaker_2    don't\n",
      "9          7.20     7.64 speaker_2    understand\n",
      "10         7.72     7.76 speaker_2    the\n",
      "11         7.92     8.12 speaker_2    terms\n",
      "12         8.20     8.24 speaker_2    that\n",
      "13         8.32     8.48 speaker_2    you're\n",
      "14         8.68     8.88 speaker_2    using\n",
      "15         8.96     9.12 speaker_2    that's\n",
      "16         9.20     9.48 speaker_2    probably\n",
      "17         9.56     9.84 speaker_2    theirs\n",
      "18         9.96    10.16 speaker_2    start\n",
      "19        10.28    10.44 speaker_2    it's\n",
      "20        10.48    10.64 speaker_2    really\n",
      "21        10.72    11.12 speaker_2    important\n",
      "22        11.20    11.24 speaker_2    that\n",
      "23        11.48    11.64 speaker_2    as\n",
      "24        12.48    12.52 speaker_2    a\n",
      "25        12.68    12.92 speaker_2    leader\n",
      "26        12.96    13.00 speaker_2    in\n",
      "27        13.04    13.08 speaker_2    the\n",
      "28        13.20    13.64 speaker_2    organization\n",
      "29        13.72    13.76 speaker_2    you\n",
      "30        13.92    14.24 speaker_2    understand\n",
      "31        14.28    14.32 speaker_2    what\n",
      "32        14.48    15.24 speaker_2    digitzation\n",
      "33        15.36    15.68 speaker_2    means\n",
      "34        16.16    16.20 speaker_2    you\n",
      "35        16.32    16.56 speaker_2    take\n",
      "36        16.60    16.64 speaker_2    the\n",
      "37        16.72    16.96 speaker_2    time\n",
      "38        17.00    17.04 speaker_2    to\n",
      "39        17.12    17.32 speaker_2    read\n",
      "40        17.44    17.80 speaker_2    widely\n",
      "41        17.84    17.88 speaker_2    in\n",
      "42        17.96    18.00 speaker_2    the\n",
      "43        18.04    18.36 speaker_2    sector\n",
      "44        18.44    18.52 speaker_2    there\n",
      "45        18.56    18.64 speaker_2    are\n",
      "46        18.68    18.72 speaker_2    a\n",
      "47        18.76    18.92 speaker_2    lot\n",
      "48        18.96    19.00 speaker_2    of\n",
      "49        19.04    19.24 speaker_2    really\n",
      "50        19.28    19.44 speaker_2    good\n",
      "51        19.48    19.80 speaker_2    books\n",
      "52        19.88    20.16 speaker_2    kevin\n",
      "53        20.24    20.56 speaker_2    kelly\n",
      "54        20.64    20.80 speaker_2    who\n",
      "55        21.36    21.68 speaker_2    started\n",
      "56        21.76    22.00 speaker_2    wide\n",
      "57        22.04    22.48 speaker_2    magazine\n",
      "58        22.52    22.60 speaker_2    has\n",
      "59        22.64    22.92 speaker_2    written\n",
      "60        22.96    23.00 speaker_2    a\n",
      "61        23.04    23.28 speaker_2    great\n",
      "62        23.68    23.92 speaker_2    great\n",
      "63        23.96    24.20 speaker_2    book\n",
      "64        24.28    24.32 speaker_2    on\n",
      "65        25.00    25.04 speaker_2    on\n",
      "66        25.24    25.52 speaker_2    various\n",
      "67        25.56    26.16 speaker_2    technologies\n",
      "68        26.32    26.36 speaker_2    i\n",
      "69        26.40    26.56 speaker_2    think\n",
      "70        26.68    27.20 speaker_2    understanding\n",
      "71        27.24    27.28 speaker_2    the\n",
      "72        27.32    27.92 speaker_2    technologies\n",
      "73        28.00    28.44 speaker_2    understanding\n",
      "74        28.48    28.64 speaker_2    what's\n",
      "75        28.68    28.84 speaker_2    out\n",
      "76        28.88    29.00 speaker_2    there\n",
      "77        29.08    29.12 speaker_2    so\n",
      "78        29.24    29.28 speaker_2    that\n",
      "79        29.64    29.68 speaker_2    you\n",
      "80        29.76    29.88 speaker_2    can\n",
      "81        29.96    30.36 speaker_2    separate\n",
      "82        30.44    30.48 speaker_2    the\n",
      "83        30.64    30.84 speaker_2    ype\n",
      "84        30.88    31.04 speaker_2    from\n",
      "85        31.08    31.12 speaker_2    the\n",
      "86        31.16    31.48 speaker_2    hope\n",
      "87        31.92    31.96 speaker_2    is\n",
      "88        32.04    32.28 speaker_2    really\n",
      "89        32.40    32.44 speaker_2    an\n",
      "90        33.00    33.44 speaker_2    important\n",
      "91        33.48    33.76 speaker_2    first\n",
      "92        33.84    34.08 speaker_2    step\n",
      "93        34.60    34.64 speaker_2    and\n",
      "94        34.76    34.96 speaker_2    then\n",
      "95        35.20    35.44 speaker_2    making\n",
      "96        35.52    35.72 speaker_2    sure\n",
      "97        35.80    35.84 speaker_2    you\n",
      "98        36.08    36.48 speaker_2    understand\n",
      "99        36.56    36.60 speaker_2    the\n",
      "100       36.64    37.08 speaker_2    relevance\n",
      "101       37.12    37.16 speaker_2    of\n",
      "102       37.20    37.24 speaker_2    that\n",
      "103       37.36    37.40 speaker_2    for\n",
      "104       37.52    37.64 speaker_2    your\n",
      "105       37.72    38.08 speaker_2    function\n",
      "106       38.16    38.20 speaker_2    and\n",
      "107       38.28    38.44 speaker_2    how\n",
      "108       38.48    38.52 speaker_2    that\n",
      "109       38.60    38.88 speaker_2    fits\n",
      "110       38.92    39.08 speaker_2    into\n",
      "111       39.12    39.24 speaker_2    your\n",
      "112       39.28    39.64 speaker_2    business\n",
      "113       40.08    40.12 speaker_2    is\n",
      "114       40.16    40.20 speaker_2    the\n",
      "115       40.28    40.56 speaker_2    second\n",
      "116       40.64    40.88 speaker_1    step\n",
      "117       41.04    41.08 speaker_1    i\n",
      "118       41.16    41.32 speaker_1    think\n",
      "119       41.44    41.48 speaker_1    to\n",
      "120       41.92    42.12 speaker_1    two\n",
      "121       42.28    42.64 speaker_1    simple\n",
      "122       43.56    44.24 speaker_1    suggestions\n",
      "123       44.44    44.48 speaker_1    you\n",
      "124       44.56    44.60 speaker_1    know\n",
      "125       44.76    44.96 speaker_1    one\n",
      "126       45.44    45.60 speaker_1    one\n",
      "127       45.80    45.84 speaker_1    is\n",
      "128       46.16    46.20 speaker_1    i\n",
      "129       46.32    46.48 speaker_1    love\n",
      "130       46.52    46.56 speaker_1    the\n",
      "131       46.60    46.84 speaker_1    phrase\n",
      "132       46.92    47.32 speaker_1    brilliant\n",
      "133       47.36    47.48 speaker_1    at\n",
      "134       47.48    47.52 speaker_1    the\n",
      "135       47.56    47.96 speaker_1    basics\n",
      "136       48.12    48.28 speaker_1    right\n",
      "137       48.52    48.56 speaker_1    so\n",
      "138       49.20    49.24 speaker_1    you\n",
      "139       49.36    49.40 speaker_1    know\n",
      "140       49.52    49.68 speaker_1    how\n",
      "141       49.76    49.88 speaker_1    can\n",
      "142       49.92    49.96 speaker_1    you\n",
      "143       50.08    50.32 speaker_1    become\n",
      "144       50.36    50.76 speaker_1    brilliant\n",
      "145       50.80    50.92 speaker_1    at\n",
      "146       50.92    50.96 speaker_1    the\n",
      "147       51.00    51.44 speaker_1    basics\n",
      "148       51.80    51.84 speaker_1    but\n",
      "149       52.08    52.36 speaker_1    beyond\n",
      "150       52.44    52.48 speaker_1    that\n",
      "151       53.80    53.84 speaker_1    you\n",
      "152       53.96    54.00 speaker_1    know\n",
      "153       54.24    54.28 speaker_1    the\n",
      "154       54.36    54.88 speaker_1    fundamental\n",
      "155       54.92    55.04 speaker_1    thing\n",
      "156       55.12    55.28 speaker_1    i've\n",
      "157       55.32    55.52 speaker_1    seen\n",
      "158       55.60    55.76 speaker_1    which\n",
      "159       55.84    56.12 speaker_1    hasn't\n",
      "160       56.24    56.52 speaker_1    changed\n",
      "161       56.72    56.76 speaker_1    is\n",
      "162       56.88    56.92 speaker_1    so\n",
      "163       57.04    57.24 speaker_1    few\n",
      "164       57.44    58.20 speaker_1    organizations\n",
      "165       58.64    58.76 speaker_1    as\n",
      "166       58.84    58.88 speaker_1    a\n",
      "167       58.92    59.16 speaker_1    first\n",
      "168       59.20    59.40 speaker_1    step\n",
      "169       59.52    59.56 speaker_1    have\n",
      "170       59.68    60.00 speaker_1    truly\n",
      "171       60.16    60.40 speaker_1    taking\n",
      "172       60.56    60.84 speaker_1    control\n",
      "173       60.92    60.96 speaker_1    of\n",
      "174       61.04    61.20 speaker_1    their\n",
      "175       61.24    61.52 speaker_1    spend\n",
      "176       61.64    61.88 speaker_1    data\n",
      "177       62.08    62.12 speaker_1    you\n",
      "178       62.20    62.24 speaker_1    know\n",
      "179       62.36    62.48 speaker_1    as\n",
      "180       62.52    62.56 speaker_1    a\n",
      "181       62.68    62.88 speaker_1    key\n",
      "182       62.96    63.20 speaker_1    first\n",
      "183       63.24    63.44 speaker_1    step\n",
      "184       63.64    63.68 speaker_1    on\n",
      "185       63.76    63.80 speaker_1    the\n",
      "186       63.88    64.24 speaker_1    digital\n",
      "187       64.28    64.96 speaker_1    transformation\n",
      "188       65.40    65.72 speaker_1    taking\n",
      "189       65.88    66.48 speaker_1    ownership\n",
      "190       66.84    66.88 speaker_1    of\n",
      "191       67.16    67.44 speaker_1    data\n",
      "192       67.96    68.00 speaker_1    and\n",
      "193       68.44    68.64 speaker_1    that's\n",
      "194       68.76    68.80 speaker_1    not\n",
      "195       69.00    69.04 speaker_1    a\n",
      "196       69.12    69.48 speaker_1    decision\n",
      "197       69.56    69.60 speaker_1    to\n",
      "198       69.76    69.88 speaker_1    use\n",
      "199       69.96    70.04 speaker_1    one\n",
      "200       70.20    70.52 speaker_1    vendor\n",
      "201       70.64    70.84 speaker_1    over\n",
      "202       70.92    71.16 speaker_1    someone\n",
      "203       71.24    71.40 speaker_1    else\n",
      "204       71.60    71.64 speaker_1    that\n",
      "205       71.80    72.08 speaker_1    says\n",
      "206       72.40    72.64 speaker_1    we're\n",
      "207       72.72    72.88 speaker_1    going\n",
      "208       72.92    72.96 speaker_1    to\n",
      "209       73.00    73.04 speaker_1    be\n",
      "210       73.20    73.60 speaker_1    completely\n",
      "211       73.68    73.92 speaker_1    data\n",
      "212       74.00    74.24 speaker_1    driven\n",
      "213       74.32    74.48 speaker_1    we're\n",
      "214       74.48    74.64 speaker_1    going\n",
      "215       74.64    74.68 speaker_1    to\n",
      "216       74.72    74.84 speaker_1    try\n",
      "217       74.88    74.92 speaker_1    and\n",
      "218       74.96    75.00 speaker_1    be\n",
      "219       75.08    75.16 speaker_1    as\n",
      "220       75.24    75.40 speaker_1    real\n",
      "221       75.44    75.64 speaker_1    time\n",
      "222       75.68    75.76 speaker_1    as\n",
      "223       75.84    76.28 speaker_1    possible\n",
      "224       76.52    76.56 speaker_1    and\n",
      "225       76.64    76.84 speaker_1    we're\n",
      "226       77.12    77.28 speaker_1    we're\n",
      "227       77.32    77.48 speaker_1    going\n",
      "228       77.48    77.52 speaker_1    to\n",
      "229       77.52    77.56 speaker_1    be\n",
      "230       77.64    77.80 speaker_1    able\n",
      "231       77.84    77.88 speaker_1    to\n",
      "232       77.96    78.24 speaker_1    explain\n",
      "233       78.32    78.36 speaker_1    that\n",
      "234       78.52    78.72 speaker_1    data\n",
      "235       78.80    78.84 speaker_1    to\n",
      "236       78.96    79.24 speaker_1    anyone\n",
      "237       79.32    79.36 speaker_1    the\n",
      "238       79.40    79.56 speaker_1    way\n",
      "239       79.60    79.72 speaker_1    they\n",
      "240       79.80    79.92 speaker_1    want\n",
      "241       79.96    80.00 speaker_1    to\n",
      "242       80.04    80.24 speaker_1    see\n",
      "243       80.28    80.32 speaker_1    it\n",
      "244       81.08    81.68 speaker_4    understand\n",
      "245       82.08    82.36 speaker_4    why\n",
      "246       82.76    83.00 speaker_4    you're\n",
      "247       83.08    83.32 speaker_4    doing\n",
      "248       83.40    83.44 speaker_4    it\n",
      "249       84.68    84.72 speaker_4    and\n",
      "250       85.00    85.04 speaker_4    the\n",
      "251       85.12    85.40 speaker_4    second\n",
      "252       85.48    85.68 speaker_4    thing\n",
      "253       85.88    85.92 speaker_4    is\n",
      "254       87.04    87.28 speaker_4    reach\n",
      "255       87.48    87.68 speaker_4    out\n",
      "256       88.60    88.64 speaker_4    to\n",
      "257       89.44    89.96 speaker_4    suppliers\n",
      "258       90.00    90.04 speaker_4    in\n",
      "259       90.12    90.16 speaker_4    the\n",
      "260       90.20    90.56 speaker_4    market\n",
      "261       90.80    91.04 speaker_4    talk\n",
      "262       91.16    91.20 speaker_4    to\n",
      "263       91.28    91.44 speaker_4    them\n",
      "264       91.56    92.16 speaker_4    collaborate\n",
      "265       92.28    92.32 speaker_4    with\n",
      "266       92.44    92.60 speaker_4    them\n",
      "267       92.88    92.92 speaker_4    you\n",
      "268       93.12    93.28 speaker_4    get\n",
      "269       93.32    93.36 speaker_4    a\n",
      "270       93.44    93.60 speaker_4    much\n",
      "271       93.64    93.88 speaker_4    better\n",
      "272       93.92    94.32 speaker_4    outcome\n",
      "273       94.96    95.20 speaker_3    think\n",
      "274       95.28    95.56 speaker_3    about\n",
      "275       95.80    95.84 speaker_3    what\n",
      "276       96.08    96.52 speaker_3    outcome\n",
      "277       96.56    96.60 speaker_3    you\n",
      "278       96.76    97.00 speaker_3    want\n",
      "279       97.16    97.32 speaker_3    at\n",
      "280       97.36    97.40 speaker_3    the\n",
      "281       97.64    97.76 speaker_3    end\n",
      "282       98.60    99.04 speaker_3    instead\n",
      "283       99.12    99.16 speaker_3    of\n",
      "284       99.44    99.80 speaker_3    thinking\n",
      "285       99.88   100.20 speaker_3    about\n",
      "286      100.96   101.00 speaker_3    the\n",
      "287      101.12   101.44 speaker_3    different\n",
      "288      101.56   102.20 speaker_3    processes\n",
      "289      102.28   102.32 speaker_3    and\n",
      "290      103.28   103.72 speaker_3    software\n",
      "291      103.80   104.12 speaker_3    names\n",
      "292      105.24   105.28 speaker_3    the\n",
      "293      105.48   105.92 speaker_3    sourcing\n",
      "294      106.00   106.20 speaker_3    being\n",
      "295      106.28   106.44 speaker_3    one\n",
      "296      106.56   106.60 speaker_3    of\n",
      "297      107.12   107.44 speaker_3    twenty\n",
      "298      108.00   108.20 speaker_3    think\n",
      "299      108.28   108.48 speaker_3    big\n",
      "300      108.64   108.68 speaker_3    and\n",
      "301      108.72   108.76 speaker_3    be\n",
      "302      108.88   109.16 speaker_3    brave\n",
      "303      109.56   109.60 speaker_3    i\n",
      "304      109.68   109.88 speaker_3    think\n",
      "305      110.88   110.92 speaker_3    and\n",
      "306      111.16   111.36 speaker_3    talk\n",
      "307      111.48   111.52 speaker_3    to\n",
      "308      112.08   112.64 speaker_3    technology\n",
      "309      112.76   113.16 speaker_3    vendors\n",
      "310      113.68   114.00 speaker_3    because\n",
      "311      114.32   114.52 speaker_3    rather\n",
      "312      114.56   114.68 speaker_3    than\n",
      "313      114.84   114.96 speaker_3    just\n",
      "314      115.04   115.36 speaker_3    sending\n",
      "315      115.40   115.52 speaker_3    them\n",
      "316      115.60   115.96 speaker_3    forms\n",
      "317      117.16   117.20 speaker_3    we\n",
      "318      117.28   117.48 speaker_3    won't\n",
      "319      117.56   117.80 speaker_3    bite\n",
      "320      117.84   117.88 speaker_3    you\n",
      "321      119.20   119.24 speaker_0    i\n",
      "322      119.36   119.52 speaker_0    think\n",
      "323      119.56   119.60 speaker_0    we\n",
      "324      119.72   120.00 speaker_0    should\n",
      "325      120.28   120.92 speaker_0    fundamentally\n",
      "326      121.00   121.16 speaker_0    all\n",
      "327      121.20   121.24 speaker_0    of\n",
      "328      121.44   121.48 speaker_0    us\n",
      "329      121.68   121.72 speaker_0    we\n",
      "330      122.00   122.20 speaker_0    think\n",
      "331      122.72   122.88 speaker_0    ow\n",
      "332      122.96   123.48 speaker_0    percurementce\n",
      "333      123.52   123.76 speaker_0    should\n",
      "334      123.80   123.84 speaker_0    be\n",
      "335      123.92   124.12 speaker_0    done\n",
      "336      125.08   125.12 speaker_0    and\n",
      "337      125.28   125.44 speaker_0    then\n",
      "338      125.60   125.84 speaker_0    start\n",
      "339      125.88   125.92 speaker_0    to\n",
      "340      126.04   126.44 speaker_0    define\n",
      "341      126.48   126.52 speaker_0    the\n",
      "342      126.64   127.28 speaker_0    functionality\n",
      "343      127.36   127.40 speaker_0    that\n",
      "344      127.56   127.60 speaker_0    we\n",
      "345      127.68   127.88 speaker_0    need\n",
      "346      128.28   128.32 speaker_0    and\n",
      "347      128.52   128.64 speaker_0    how\n",
      "348      128.72   128.76 speaker_0    we\n",
      "349      128.84   128.96 speaker_0    can\n",
      "350      129.04   129.24 speaker_0    make\n",
      "351      129.24   129.28 speaker_0    this\n",
      "352      129.44   129.68 speaker_0    work\n",
      "353      130.72   130.76 speaker_0    what\n",
      "354      130.92   130.96 speaker_0    we\n",
      "355      131.08   131.12 speaker_0    do\n",
      "356      131.36   131.64 speaker_0    today\n",
      "357      131.92   131.96 speaker_0    is\n",
      "358      132.20   133.00 speaker_0    absolutely\n",
      "359      133.64   133.92 speaker_0    wong\n",
      "360      135.52   135.56 speaker_0    we\n",
      "361      135.72   135.88 speaker_0    don't\n",
      "362      136.00   136.04 speaker_0    like\n",
      "363      136.16   136.20 speaker_0    it\n",
      "364      137.00   137.28 speaker_0    cure\n",
      "365      137.32   137.56 speaker_0    people\n",
      "366      137.56   137.76 speaker_0    don't\n",
      "367      137.84   137.88 speaker_0    like\n",
      "368      138.00   138.04 speaker_0    it\n",
      "369      138.72   138.88 speaker_0    our\n",
      "370      138.96   139.32 speaker_0    colleagues\n",
      "371      139.40   139.60 speaker_0    don't\n",
      "372      139.68   139.72 speaker_0    like\n",
      "373      139.88   139.92 speaker_0    it\n",
      "374      140.52   140.92 speaker_0    nobody\n",
      "375      141.04   141.32 speaker_0    wants\n",
      "376      141.40   141.44 speaker_0    it\n",
      "377      142.04   142.08 speaker_0    and\n",
      "378      142.16   142.32 speaker_0    we're\n",
      "379      142.40   142.68 speaker_0    spending\n",
      "380      142.76   142.80 speaker_0    a\n",
      "381      142.84   143.04 speaker_0    huge\n",
      "382      143.08   143.36 speaker_0    amount\n",
      "383      143.40   143.44 speaker_0    of\n",
      "384      143.56   143.80 speaker_0    money\n",
      "385      144.48   144.52 speaker_0    for\n",
      "386      144.68   144.80 speaker_0    no\n",
      "387      144.92   145.20 speaker_0    reason\n",
      "\n",
      "--- SUMMARY: 388 words, 5 speakers ---\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'Index':<6} {'Start':>8} {'End':>8} {'Speaker':<12} {'Word'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "words_info = trans_info_dict[file_key]['words']\n",
    "for idx, w in enumerate(words_info):\n",
    "    print(f\"{idx:<6} {w['start_time']:>8.2f} {w['end_time']:>8.2f} {w['speaker']:<12} {w['word']}\")\n",
    "\n",
    "speakers = set(w['speaker'] for w in words_info)\n",
    "print(f\"\\n--- SUMMARY: {len(words_info)} words, {len(speakers)} speakers ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 490623,
     "sourceId": 916741,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1877225,
     "sourceId": 3085190,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5115624,
     "sourceId": 8559016,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
